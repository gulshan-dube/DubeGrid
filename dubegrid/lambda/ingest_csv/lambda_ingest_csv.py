import boto3
import csv
import io
import urllib.parse   # â¬… NEW: to decode the S3 key

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key    = event['Records'][0]['s3']['object']['key']

    # ğŸ” Debug incoming S3 event values
    print(f"DEBUG â€” Incoming bucket: {bucket}")
    print(f"DEBUG â€” Incoming key:    {key}")

    # âœ… Decode URLâ€‘encoded characters (e.g. %3D â†’ =)
    key = urllib.parse.unquote_plus(key)

    s3 = boto3.client('s3')
    response = s3.get_object(Bucket=bucket, Key=key)
    content = response['Body'].read().decode('utf-8')
    reader = csv.DictReader(io.StringIO(content))

    # ğŸ”— Connect to DynamoDB
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('DubeGridAssetData')

    for i, row in enumerate(reader, start=1):
        if i <= 5:  # show only first 5 rows for sanity check
            print(row)

        # ğŸ“ Insert each row into DynamoDB
        table.put_item(Item={
            'asset_id': row['asset_id'],
            'timestamp': row['timestamp'],
            'value': row['value']  # adjust based on your actual CSV columns
        })

    print(f"âœ… Processed {i} rows and inserted into DynamoDB")
